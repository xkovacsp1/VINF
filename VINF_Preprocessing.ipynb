{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VINF_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omhmaqd4uLrq"
      },
      "source": [
        "# <center>Searching in scientific papers</center>\n",
        "##  <center>Patrik Kovács</center>\n",
        "###  <center>Subject: Information retrieval </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMqqvYurbQH_"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef3HQgKBbPhR",
        "outputId": "b1ada693-8993-4ff7-f8d8-332ad15495f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D51CoNk9bPqo"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwwKoyK8Ctlf"
      },
      "source": [
        "# Used libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Pe6dObn1_r",
        "outputId": "647140d6-5b86-4566-a81a-026c97ab289c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "%pip install ijson==3.1.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ijson==3.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/db/10adc9c596867beffc14ca6be630657083729833b24f4986690eecc063e1/ijson-3.1.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: ijson\n",
            "Successfully installed ijson-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wht1aEI7bayq",
        "outputId": "326906b0-92ab-49ee-97a0-cce44173bfb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "import ijson\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq6fmRX0qia2"
      },
      "source": [
        "#downloaded = drive.CreateFile({'id':'1JRSqi4HDhIHXtOBo4P6M30qrX2o-IS2z'}) \n",
        "#downloaded.GetContentFile('arxiv-metadata-oai-snapshot.jsonb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4M1KzpZss2v"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1-6gFZm7ZAKlMVO_K3qpdJpU7TQKYZPZX'}) \n",
        "downloaded.GetContentFile('papers.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHDMPom9FsbS"
      },
      "source": [
        "#with open('papers.json') as f:\n",
        "  #data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEpqGlViGdJD"
      },
      "source": [
        "#with open('papers_medium.json', 'w') as json_file:\n",
        "  #json.dump(data[:1000], json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqFkqlUmFhH4"
      },
      "source": [
        "#df=pd.read_json('arxiv-metadata-oai-snapshot.json',lines = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhz3Q62Dba-P"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTzNn29Unyn3"
      },
      "source": [
        "#df=pd.read_json('arxiv-metadata-oai-snapshot.json',lines = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaYJFqkZnm6r",
        "outputId": "2c088f9c-4abf-438d-de8e-b701e48a1193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import ijson\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "docs = []\n",
        "count = 0\n",
        "remove=False\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print(\"module %s loaded\" % module_url)\n",
        "\n",
        "id = None\n",
        "submitter = None\n",
        "authors = None\n",
        "title = None\n",
        "comments = None\n",
        "journal_ref = None\n",
        "doi = None\n",
        "report_no = None\n",
        "categories = None\n",
        "license = None\n",
        "abstract = None\n",
        "versions = None\n",
        "update_date = None\n",
        "authors_parsed = None\n",
        "pages = None\n",
        "figures = None\n",
        "latest_version_date = None\n",
        "latest_version = None\n",
        "list_of_authors = None\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
        "    return ' '.join(no_stopword_text)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # remove backslash-apostrophe\n",
        "    text = re.sub(\"\\'\", \"\", text)\n",
        "    # remove everything except alphabets and numbers\n",
        "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    # remove whitespaces\n",
        "    text = ' '.join(text.split())\n",
        "    # convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def format_sentence_encoder_result_to_array(arr):\n",
        "    return str(np.array(arr[0]).tolist())\n",
        "\n",
        "\n",
        "def append_to_json(_dict, path):\n",
        "    with open(path, 'ab+') as f:\n",
        "        f.seek(0, 2)  # Go to the end of file\n",
        "        if f.tell() == 0:  # Check if file is empty\n",
        "            f.write(json.dumps(_dict).encode())  # If empty, write an array\n",
        "        else:\n",
        "            pos = f.seek(-1, 2)\n",
        "            f.truncate()  # Remove the last character, open the array\n",
        "            f.write(' , '.encode())  # Write the separator\n",
        "            # Write after from [ character\n",
        "            f.write(json.dumps(_dict).encode()[1:])\n",
        "\n",
        "\n",
        "def get_page_number(str):\n",
        "    try:\n",
        "        found = re.search('([0-9]+) +[pP]ages?', str).group(1)\n",
        "    except AttributeError:\n",
        "        found = 'No data'\n",
        "    return found\n",
        "\n",
        "\n",
        "def get_figure_number(str):\n",
        "    try:\n",
        "        found = re.search('([0-9]+) +[fF]igures?', str).group(1)\n",
        "    except AttributeError:\n",
        "        found = 'No data'\n",
        "    return found\n",
        "\n",
        "\n",
        "def categories_to_list_of_strings(categories):\n",
        "    return str(categories.split(' '))\n",
        "\n",
        "\n",
        "def get_version_date(versions):\n",
        "    versions = eval(versions)\n",
        "    return str(versions[-1]['created'])\n",
        "\n",
        "\n",
        "def get_version_number(versions):\n",
        "    versions = eval(versions)\n",
        "    return str(versions[-1]['version'])\n",
        "\n",
        "\n",
        "def get_list_of_authors(authors):\n",
        "    authors = eval(authors)\n",
        "    res = list(map(' '.join, authors))\n",
        "    return str(list(map(str.strip, res)))\n",
        "\n",
        "\n",
        "for prefix, the_type, value in ijson.parse(open('papers_small.json')):\n",
        "    #beginning of new paper \n",
        "    if(prefix == 'item.id'):\n",
        "        id = value\n",
        "        remove = False\n",
        "    if(remove == False):\n",
        "        if(prefix == 'item.submitter'):\n",
        "            submitter = value\n",
        "        if(prefix == 'item.authors'):\n",
        "            authors = value\n",
        "        if(prefix == 'item.title'):\n",
        "            title = value\n",
        "        if(prefix == 'item.comments'):\n",
        "            pages = get_page_number(value)\n",
        "            figures = get_figure_number(value)\n",
        "        if(prefix == 'item.journal-ref'):\n",
        "            journal_ref = value\n",
        "        if(prefix == 'item.doi'):\n",
        "            doi = value\n",
        "        if(prefix == 'item.report-no'):\n",
        "            report_no = value\n",
        "        if(prefix == 'item.categories'):\n",
        "            categories = categories_to_list_of_strings(value)\n",
        "        if(prefix == 'item.license'):\n",
        "            license = value\n",
        "        if(prefix == 'item.abstract'):\n",
        "            # filter out not accepted papers\n",
        "            if(bool(re.search('[pP]aper.*[wW]ithdrawn|^[Ww]ithdrawn', value))):\n",
        "                remove = True\n",
        "                continue\n",
        "            # vectorize abstract\n",
        "            abstract = clean_text(value)\n",
        "            abstract = remove_stopwords(abstract)\n",
        "            abstract = model([abstract])           # google sentence encoder\n",
        "            abstract = format_sentence_encoder_result_to_array(abstract)\n",
        "        if(prefix == 'item.versions'):\n",
        "            latest_version_date = get_version_date(value)\n",
        "            latest_version = get_version_number(value)\n",
        "        if(prefix == 'item.update_date'):\n",
        "            update_date = value\n",
        "        if(prefix == 'item.authors_parsed'):\n",
        "            list_of_authors = get_list_of_authors(value)\n",
        "        if(id and submitter and title and journal_ref and doi and report_no and categories and license and abstract and update_date\n",
        "        and pages and figures and latest_version_date and latest_version and list_of_authors):\n",
        "\n",
        "            body = {\n",
        "                \"id\": id,\n",
        "                \"submitter\": submitter,\n",
        "                \"title\": title,\n",
        "                \"journal_ref\": journal_ref,\n",
        "                \"doi\": doi,\n",
        "                \"report_no\": report_no,\n",
        "                \"categories\": categories, \n",
        "                \"license\": license,\n",
        "                \"abstract\": abstract,\n",
        "                \"update_date\": update_date,\n",
        "                \"pages\": pages,\n",
        "                \"figures\": figures,\n",
        "                \"latest_version_date\": latest_version_date,\n",
        "                \"latest_version\": latest_version,\n",
        "                \"list_of_authors\": list_of_authors\n",
        "            }\n",
        "            id = None\n",
        "            submitter = None\n",
        "            title = None\n",
        "            journal_ref = None\n",
        "            doi = None\n",
        "            report_no = None\n",
        "            categories = None\n",
        "            license = None\n",
        "            abstract = None\n",
        "            update_date = None\n",
        "            pages = None\n",
        "            figures = None\n",
        "            latest_version_date = None\n",
        "            latest_version = None\n",
        "            list_of_authors = None\n",
        "            docs.append(body)\n",
        "\n",
        "            count += 1\n",
        "            if count % 100 == 0:\n",
        "                append_to_json(docs, 'res_small.json')\n",
        "                docs = []\n",
        "                print(\"saved {} documents.\".format(count))\n",
        "\n",
        "\n",
        "if docs:\n",
        "    append_to_json(docs, 'res_small.json')\n",
        "    docs = []\n",
        "    print(\"saved {} documents.\".format(count))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n",
            "saved 100 documents.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKsLUtqDnnOr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-4WqxHMnnfH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}