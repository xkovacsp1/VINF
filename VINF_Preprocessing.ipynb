{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VINF_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omhmaqd4uLrq"
      },
      "source": [
        "# <center>Searching in scientific papers</center>\n",
        "##  <center>Patrik Kovács</center>\n",
        "###  <center>Subject: Information retrieval </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMqqvYurbQH_"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef3HQgKBbPhR",
        "outputId": "34e1de7a-77dc-4c58-9778-ead296ba88ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D51CoNk9bPqo"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwwKoyK8Ctlf"
      },
      "source": [
        "# Used libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Pe6dObn1_r",
        "outputId": "b5030d5b-941d-418a-ba1b-e6361378d018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "%pip install ijson==3.1.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ijson==3.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/db/10adc9c596867beffc14ca6be630657083729833b24f4986690eecc063e1/ijson-3.1.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 71kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: ijson\n",
            "Successfully installed ijson-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wht1aEI7bayq",
        "outputId": "35ff0395-d196-4311-b851-fafaf8f68e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "import ijson\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq6fmRX0qia2"
      },
      "source": [
        "#downloaded = drive.CreateFile({'id':'1JRSqi4HDhIHXtOBo4P6M30qrX2o-IS2z'}) \n",
        "#downloaded.GetContentFile('arxiv-metadata-oai-snapshot.jsonb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4M1KzpZss2v"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1-6gFZm7ZAKlMVO_K3qpdJpU7TQKYZPZX'}) \n",
        "downloaded.GetContentFile('papers.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHDMPom9FsbS"
      },
      "source": [
        "#with open('papers.json') as f:\n",
        "  #data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEpqGlViGdJD"
      },
      "source": [
        "#with open('papers_medium.json', 'w') as json_file:\n",
        "  #json.dump(data[:1000], json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqFkqlUmFhH4"
      },
      "source": [
        "#df=pd.read_json('arxiv-metadata-oai-snapshot.json',lines = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhz3Q62Dba-P"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTzNn29Unyn3"
      },
      "source": [
        "#df=pd.read_json('arxiv-metadata-oai-snapshot.json',lines = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaYJFqkZnm6r",
        "outputId": "ce511485-78dc-434f-df32-f414a606c6cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import ijson\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "docs = []\n",
        "count = 0\n",
        "remove = False\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print(\"module %s loaded\" % module_url)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
        "    return ' '.join(no_stopword_text)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # remove backslash-apostrophe\n",
        "    text = re.sub(\"\\'\", \"\", text)\n",
        "    # remove everything except alphabets and numbers\n",
        "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    # remove whitespaces\n",
        "    text = ' '.join(text.split())\n",
        "    # convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def format_sentence_encoder_result_to_array(arr):\n",
        "    return str(np.array(arr[0]).tolist())\n",
        "\n",
        "\n",
        "def append_to_json(_dict, path):\n",
        "    with open(path, 'ab+') as f:\n",
        "        f.seek(0, 2)  # Go to the end of file\n",
        "        if f.tell() == 0:  # Check if file is empty\n",
        "            f.write(json.dumps(_dict).encode())  # If empty, write an array\n",
        "        else:\n",
        "            pos = f.seek(-1, 2)\n",
        "            f.truncate()  # Remove the last character, open the array\n",
        "            f.write(' , '.encode())  # Write the separator\n",
        "            # Write after from [ character\n",
        "            f.write(json.dumps(_dict).encode()[1:])\n",
        "\n",
        "\n",
        "def get_page_number(str):\n",
        "    try:\n",
        "        found = re.search('([0-9]+) +[pP]ages?', str).group(1)\n",
        "    except AttributeError:\n",
        "        found = 'No data'\n",
        "    return found\n",
        "\n",
        "\n",
        "def get_figure_number(str):\n",
        "    try:\n",
        "        found = re.search('([0-9]+) +[fF]igures?', str).group(1)\n",
        "    except AttributeError:\n",
        "        found = 'No data'\n",
        "    return found\n",
        "\n",
        "\n",
        "def categories_to_list_of_strings(categories):\n",
        "    return str(categories.split(' '))\n",
        "\n",
        "\n",
        "def get_version_date(versions):\n",
        "    versions = eval(versions)\n",
        "    return str(versions[-1]['created'])\n",
        "\n",
        "\n",
        "def get_version_number(versions):\n",
        "    versions = eval(versions)\n",
        "    return str(versions[-1]['version'])\n",
        "\n",
        "\n",
        "def get_list_of_authors(authors):\n",
        "    authors = eval(authors)\n",
        "    res = list(map(' '.join, authors))\n",
        "    return str(list(map(str.strip, res)))\n",
        "\n",
        "\n",
        "with open('papers.json', 'rb') as data:\n",
        "    for obj in ijson.items(data, 'item'):\n",
        "        # filter out not accepted papers\n",
        "        if(bool(re.search('[pP]aper.*[wW]ithdrawn|^[Ww]ithdrawn', obj['abstract']))):\n",
        "            continue\n",
        "         # vectorize abstract\n",
        "        abstract = clean_text(obj['abstract'])\n",
        "        abstract = remove_stopwords(abstract)\n",
        "        abstract = model([abstract])           # google sentence encoder\n",
        "        abstract = format_sentence_encoder_result_to_array(abstract)\n",
        "        pages = get_page_number(obj['comments'])\n",
        "        figures = get_figure_number(obj['comments'])\n",
        "        categories = categories_to_list_of_strings(obj['categories'])\n",
        "        latest_version_date = get_version_date(obj['versions'])\n",
        "        latest_version = get_version_number(obj['versions'])\n",
        "        list_of_authors = get_list_of_authors(obj['authors_parsed'])\n",
        "\n",
        "        body = {\n",
        "            \"id\": obj['id'],\n",
        "            \"submitter\": obj['submitter'],\n",
        "            \"title\": obj['title'],\n",
        "            \"journal_ref\": obj['journal-ref'],\n",
        "            \"doi\": obj['doi'],\n",
        "            \"report_no\": obj['report-no'],\n",
        "            \"categories\": categories,\n",
        "            \"license\": obj['license'],\n",
        "            \"abstract\": abstract,\n",
        "            \"update_date\": obj['update_date'],\n",
        "            \"pages\": pages,\n",
        "            \"figures\": figures,\n",
        "            \"latest_version_date\": latest_version_date,\n",
        "            \"latest_version\": latest_version,\n",
        "            \"list_of_authors\": list_of_authors\n",
        "        }\n",
        "        docs.append(body)\n",
        "        count += 1\n",
        "        if count % 100 == 0:\n",
        "            append_to_json(docs, 'res.json')\n",
        "            docs = []\n",
        "            print(\"saved {} documents.\".format(count))\n",
        "\n",
        "if docs:\n",
        "    append_to_json(docs, 'res.json')\n",
        "    docs = []\n",
        "    print(\"saved {} documents.\".format(count))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n",
            "saved 100 documents.\n",
            "saved 200 documents.\n",
            "saved 300 documents.\n",
            "saved 400 documents.\n",
            "saved 500 documents.\n",
            "saved 600 documents.\n",
            "saved 700 documents.\n",
            "saved 800 documents.\n",
            "saved 900 documents.\n",
            "saved 997 documents.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKsLUtqDnnOr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-4WqxHMnnfH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}