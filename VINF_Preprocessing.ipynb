{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VINF_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omhmaqd4uLrq"
      },
      "source": [
        "# <center>Searching in scientific papers</center>\n",
        "##  <center>Patrik Kovács</center>\n",
        "###  <center>Subject: Information retrieval </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMqqvYurbQH_"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef3HQgKBbPhR",
        "outputId": "d95ab0c2-6790-468d-ddfa-5b374a945bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D51CoNk9bPqo"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwwKoyK8Ctlf"
      },
      "source": [
        "# Used libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Pe6dObn1_r",
        "outputId": "527633c2-bba6-48e7-f3c0-0dd9941aad08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "%pip install ijson==3.1.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ijson==3.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/db/10adc9c596867beffc14ca6be630657083729833b24f4986690eecc063e1/ijson-3.1.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.5MB/s \n",
            "\u001b[?25hInstalling collected packages: ijson\n",
            "Successfully installed ijson-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wht1aEI7bayq",
        "outputId": "97d3366e-9991-4102-9e98-53dff989594b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "import ijson\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq6fmRX0qia2"
      },
      "source": [
        "#downloaded = drive.CreateFile({'id':'1JRSqi4HDhIHXtOBo4P6M30qrX2o-IS2z'}) \n",
        "#downloaded.GetContentFile('arxiv-metadata-oai-snapshot.jsonb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4M1KzpZss2v"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1-6gFZm7ZAKlMVO_K3qpdJpU7TQKYZPZX'}) \n",
        "downloaded.GetContentFile('papers.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhz3Q62Dba-P"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTzNn29Unyn3"
      },
      "source": [
        "#df=pd.read_json('arxiv-metadata-oai-snapshot.json',lines = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaYJFqkZnm6r",
        "outputId": "ca238230-56aa-4cc5-8a5b-d09d075115ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        }
      },
      "source": [
        "import ijson\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "docs = []\n",
        "count = 0\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)\n",
        "print(\"module %s loaded\" % module_url)\n",
        "\n",
        "id = None\n",
        "submitter = None\n",
        "authors = None\n",
        "title = None\n",
        "comments = None\n",
        "journal_ref = None\n",
        "doi = None\n",
        "report_no = None\n",
        "categories = None\n",
        "license = None\n",
        "abstract = None\n",
        "versions = None\n",
        "update_date = None\n",
        "authors_parsed = None\n",
        "pages = None\n",
        "figures = None\n",
        "latest_version_date = None\n",
        "latest_version = None\n",
        "list_of_authors = None\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
        "    return ' '.join(no_stopword_text)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # remove backslash-apostrophe\n",
        "    text = re.sub(\"\\'\", \"\", text)\n",
        "    # remove everything except alphabets and numbers\n",
        "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    # remove whitespaces\n",
        "    text = ' '.join(text.split())\n",
        "    # convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def format_sentence_encoder_result_to_array(arr):\n",
        "    return str(np.array(arr[0]).tolist())\n",
        "\n",
        "\n",
        "def append_to_json(_dict, path):\n",
        "    with open(path, 'ab+') as f:\n",
        "        f.seek(0, 2)  # Go to the end of file\n",
        "        if f.tell() == 0:  # Check if file is empty\n",
        "            f.write(json.dumps(_dict).encode())  # If empty, write an array\n",
        "        else:\n",
        "            pos = f.seek(-1, 2)\n",
        "            f.truncate()  # Remove the last character, open the array\n",
        "            f.write(' , '.encode())  # Write the separator\n",
        "            # Write after from [ character\n",
        "            f.write(json.dumps(_dict).encode()[1:])\n",
        "\n",
        "\n",
        "def get_page_number(str):\n",
        "    try:\n",
        "        found = re.search('([0-9]+) +[pP]ages?', str).group(1)\n",
        "    except AttributeError:\n",
        "        found = 'No data'\n",
        "    return found\n",
        "\n",
        "\n",
        "def get_figure_number(str):\n",
        "    try:\n",
        "        found = re.search('([0-9]+) +[fF]igures?', str).group(1)\n",
        "    except AttributeError:\n",
        "        found = 'No data'\n",
        "    return found\n",
        "\n",
        "\n",
        "def categories_to_list_of_strings(categories):\n",
        "    return str(categories.split(' '))\n",
        "\n",
        "\n",
        "def get_version_date(versions):\n",
        "    versions = eval(versions)\n",
        "    return str(versions[-1]['created'])\n",
        "\n",
        "\n",
        "def get_version_number(versions):\n",
        "    versions = eval(versions)\n",
        "    return str(versions[-1]['version'])\n",
        "\n",
        "\n",
        "def get_list_of_authors(authors):\n",
        "    authors = eval(authors)\n",
        "    res = list(map(' '.join, authors))\n",
        "    return str(list(map(str.strip, res)))\n",
        "\n",
        "\n",
        "for prefix, the_type, value in ijson.parse(open('papers.json')):\n",
        "    if(prefix == 'item.abstract'):\n",
        "        # filter out not accepted papers\n",
        "        if(bool(re.search('[pP]aper.*[wW]ithdrawn|^[Ww]ithdrawn', value))):\n",
        "            continue\n",
        "        #vectorize abstract\n",
        "        abstract = clean_text(value)\n",
        "        abstract = remove_stopwords(abstract)\n",
        "        abstract = model([abstract])           # google sentence encoder\n",
        "        abstract = format_sentence_encoder_result_to_array(abstract)\n",
        "        # abstract=value\n",
        "    if(prefix == 'item.id'):\n",
        "        id = value\n",
        "    if(prefix == 'item.submitter'):\n",
        "        submitter = value\n",
        "    if(prefix == 'item.authors'):\n",
        "        authors = value\n",
        "    if(prefix == 'item.title'):\n",
        "        title = value\n",
        "    if(prefix == 'item.comments'):\n",
        "        pages = get_page_number(value)\n",
        "        figures = get_figure_number(value)\n",
        "        # comments=value\n",
        "    if(prefix == 'item.journal-ref'):\n",
        "        journal_ref = value\n",
        "    if(prefix == 'item.doi'):\n",
        "        doi = value\n",
        "    if(prefix == 'item.report-no'):\n",
        "        report_no = value\n",
        "    if(prefix == 'item.categories'):\n",
        "        categories = categories_to_list_of_strings(value)\n",
        "    if(prefix == 'item.license'):\n",
        "        license = value\n",
        "    if(prefix == 'item.versions'):\n",
        "        latest_version_date = get_version_date(value)\n",
        "        latest_version = get_version_number(value)\n",
        "        # versions=value\n",
        "    if(prefix == 'item.update_date'):\n",
        "        update_date = value\n",
        "    if(prefix == 'item.authors_parsed'):\n",
        "        list_of_authors = get_list_of_authors(value)\n",
        "        # list_of_authors=value\n",
        "    if(id and submitter and title and journal_ref and doi and report_no and categories and license and abstract and update_date\n",
        "       and pages and figures and latest_version_date and latest_version and list_of_authors):\n",
        "        body = {\n",
        "            \"id\": id,\n",
        "            \"submitter\": submitter,\n",
        "            # \"authors\":authors,\n",
        "            \"title\": title,\n",
        "            # \"comments\":comments,\n",
        "            \"journal_ref\": journal_ref,\n",
        "            \"doi\": doi,\n",
        "            \"report_no\": report_no,\n",
        "            \"categories\": categories,  # mozno sem dat eval\n",
        "            \"license\": license,\n",
        "            \"abstract\": abstract,\n",
        "            # \"versions\":versions,\n",
        "            \"update_date\": update_date,\n",
        "            # \"authors_parsed\":authors_parsed,\n",
        "            \"pages\": pages,\n",
        "            \"figures\": figures,\n",
        "            \"latest_version_date\": latest_version_date,\n",
        "            \"latest_version\": latest_version,\n",
        "            \"list_of_authors\": list_of_authors\n",
        "        }\n",
        "        docs.append(body)\n",
        "        id = None\n",
        "        submitter = None\n",
        "        # authors=None\n",
        "        title = None\n",
        "        # comments=None\n",
        "        journal_ref = None\n",
        "        doi = None\n",
        "        report_no = None\n",
        "        categories = None\n",
        "        license = None\n",
        "        abstract = None\n",
        "        # versions=None\n",
        "        update_date = None\n",
        "        # authors_parsed=None\n",
        "        pages = None\n",
        "        figures = None\n",
        "        latest_version_date = None\n",
        "        latest_version = None\n",
        "        list_of_authors = None\n",
        "\n",
        "        count += 1\n",
        "        if count % 10 == 0:\n",
        "            append_to_json(docs, 'res.json')\n",
        "            docs = []\n",
        "            print(\"saved {} documents.\".format(count))\n",
        "\n",
        "\n",
        "if docs:\n",
        "    append_to_json(docs, 'res.json')\n",
        "    docs = []\n",
        "    print(\"saved {} documents.\".format(count))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n",
            "saved 10 documents.\n",
            "saved 20 documents.\n",
            "saved 30 documents.\n",
            "saved 40 documents.\n",
            "saved 50 documents.\n",
            "saved 60 documents.\n",
            "saved 70 documents.\n",
            "saved 80 documents.\n",
            "saved 90 documents.\n",
            "saved 100 documents.\n",
            "saved 110 documents.\n",
            "saved 120 documents.\n",
            "saved 130 documents.\n",
            "saved 140 documents.\n",
            "saved 150 documents.\n",
            "saved 160 documents.\n",
            "saved 170 documents.\n",
            "saved 180 documents.\n",
            "saved 190 documents.\n",
            "saved 200 documents.\n",
            "saved 210 documents.\n",
            "saved 220 documents.\n",
            "saved 230 documents.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0cb03e846bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mabstract\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mabstract\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mabstract\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mabstract\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_sentence_encoder_result_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m#abstract=value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKsLUtqDnnOr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-4WqxHMnnfH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}